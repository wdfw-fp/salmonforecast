forecasts=rp$all_mods %>% group_by(year) %>% mutate(rank=rank(MAPE)) %>% ungroup,
series = dat,
TY_ensemble = TY_ensemble,
k = k,
slide = 15,
num_models=num_models,
stack_metric=stack_metric
)
expect_true(!is.null(ens))
# Assuming 'ens' is your data frame
print(head(ens, n = 10))
ensemble <- function(forecasts, series, TY_ensemble, k, slide, num_models, stack_metric, stretch = FALSE) {
yrrange <- forecasts %>%
dplyr::summarise(minyr = min(year), maxyr = max(year)) %>%
unlist()
maxdata_year <- forecasts %>%
dplyr::filter(!is.na(abundance)) %>%
dplyr::summarise(max(year)) %>%
unlist() %>%
purrr::pluck()
ensembles <- NULL
for (i in (yrrange[2] - TY_ensemble):maxdata_year) {
if(stretch){
#Xiaotian fill this in
}else{
years <- seq(to = i, length.out = slide)
}
tdat <- forecasts %>%
dplyr::filter(
model %in% c(forecasts %>%
dplyr::filter(year == i + 1, rank <= num_models) %>%
dplyr::pull(model))
) %>%
dplyr::filter(year %in% years) %>%
dplyr::left_join(series, by = "year") %>%
dplyr::select(year, model, predicted_abundance, abundance = abundance.x) %>%
dplyr::mutate(error = abundance - predicted_abundance) %>%
dplyr::filter(!is.na(error)) %>%
dplyr::group_by(model) %>%
dplyr::summarise(RMSE = sqrt(mean(error^2)),
MAPE = mean(abs(error / abundance)) * 100,
MSA = 100 * (exp(mean(abs(log(abundance / predicted_abundance)))) - 1)
) %>%
dplyr::arrange(MSA) %>%
dplyr::mutate(MSA_weight = (1 / MSA)^k / sum((1 / MSA)^k),
RMSE_weight = (1 / RMSE)^k / sum((1 / RMSE)^k),
MAPE_weight = (1 / MAPE)^k / sum((1 / MAPE)^k)
)
modelcnt <- num_models
stackdat <- forecasts %>%
dplyr::filter(
model %in% c(forecasts %>%
dplyr::filter(year == i + 1, rank <= num_models) %>%
dplyr::pull(model))
) %>%
dplyr::filter(year %in% years) %>%
tidyr::pivot_wider(names_from = model, values_from = predicted_abundance, id_cols = year) %>%
dplyr::left_join(series %>% dplyr::select(year, abundance)) %>%
ungroup()
stack_weights <- find_stack_weights(tau = 1,
n = 10000,
metric = stack_metric,
initial_weights = rep(1 / modelcnt, modelcnt),
preds = stackdat %>%
dplyr::filter(!is.na(abundance)) %>%
dplyr::select(!abundance & !year) %>%
as.matrix(),
obs = stackdat %>%
dplyr::filter(!is.na(abundance)) %>%
dplyr::select(abundance & !year) %>%
as.matrix()
)
stacking_weights <- data.frame("Stacking_weight" = as.vector(round(unlist(stack_weights[[1]]), 4)))
stacking_weights$model <- colnames(stackdat)[!colnames(stackdat) %in% c("year", "abundance")]
tdat <- tdat %>%
dplyr::left_join(stacking_weights)
tdat2 <- forecasts %>%
dplyr::filter(
model %in% c(forecasts %>%
dplyr::filter(year == i + 1, rank <= num_models) %>%
dplyr::pull(model))
) %>%
dplyr::filter(year == max(years) + 1) %>%
tidyr::pivot_longer(names_to = "Parameter",
cols = c("predicted_abundance", "Lo 95", "Hi 95", "Lo 50", "Hi 50"),
values_to = "value") %>%
dplyr::left_join(tdat %>% dplyr::select(model, MSA_weight:Stacking_weight)) %>%
dplyr::mutate(MSA_weighted = value * MSA_weight,
RMSE_weighted = value * RMSE_weight,
MAPE_weighted = value * MAPE_weight,
Stack_weighted = value * Stacking_weight
) %>%
dplyr::group_by(year, Parameter) %>%
dplyr::summarise(MSA_weighted = sum(MSA_weighted),
RMSE_weighted = sum(RMSE_weighted),
MAPE_weighted = sum(MAPE_weighted),
Stack_weighted = sum(Stack_weighted)
) %>%
tidyr::pivot_longer(names_to = "model",
cols = c("MSA_weighted", "RMSE_weighted", "MAPE_weighted", "Stack_weighted"),
values_to = "value") %>%
tidyr::pivot_wider(id_cols = c("year", "model"), names_from = Parameter, values_from = value)
# # Add "Best individual" rows with correct performance metrics
# tdat2 <- dplyr::bind_rows(
#   tdat2,
#   forecasts %>%
#     dplyr::filter(
#       model %in% c(forecasts %>%
#                      dplyr::filter(year == i + 1, rank <= num_models) %>%
#                      dplyr::pull(model))
#     ) %>%
#     dplyr::filter(year == max(years) + 1) %>%
#     dplyr::mutate(model = "Best individual") %>%
# dplyr::group_by(year, model) %>%
# dplyr::summarise(
#   MSA_weighted = mean(MSA_weight, na.rm = TRUE),
#   RMSE_weighted = mean(RMSE_weight, na.rm = TRUE),
#   MAPE_weighted = mean(MAPE_weight, na.rm = TRUE),
#   Stack_weighted = mean(Stacking_weight, na.rm = TRUE)
# )
# )
ensembles <- dplyr::bind_rows(ensembles, tdat2)
}
# function to evaluate forecast skill ()
evaluate_forecasts2 <- function(forecasts, observations) {
forecast_skill <- forecasts %>%
# left_join(observations,by=c("Year","runsize_obs"))%>%
dplyr::select(year, model, abundance, predicted_abundance) %>%
dplyr::mutate(error = predicted_abundance - abundance) %>%
dplyr::filter(!is.na(error)) %>%
dplyr::group_by(model) %>%
dplyr::summarise(
MAPE = mean(abs(error / abundance)) * 100,
RMSE = sqrt(mean(error^2)),
MSA = 100 * (exp(mean(abs(log(abundance / predicted_abundance)))) - 1)
) %>%
dplyr::arrange(MAPE)
return(forecast_skill)
}
forecast_skill <- evaluate_forecasts2(
forecasts = dplyr::bind_rows(forecasts, ensembles %>%
dplyr::left_join(series)) %>%
dplyr::filter(year > (yrrange[2] - TY_ensemble)),
observations = series
)
forecasts2 <- dplyr::bind_rows(forecasts, ensembles %>% dplyr::left_join(series)) %>%
dplyr::filter(year > (yrrange[2] - TY_ensemble)) %>%
dplyr::mutate(error = predicted_abundance - abundance,
pct_error = scales::percent(error / abundance)
) %>%
# dplyr::left_join(tdat %>% dplyr::select(model, Stacking_weight))
dplyr::left_join(tdat %>% dplyr::select(model, Stacking_weight)) %>%
dplyr::group_by(model) %>%
dplyr::summarize(
performance_metrics = calculate_performance_metrics(predicted_abundance, abundance),
Stacking_weight = mean(Stacking_weight, na.rm = TRUE)
) %>%
dplyr::ungroup() %>% unpack(cols=performance_metrics)
results <- list(
final_model_weights = tdat,
forecast_skill = forecast_skill,
ensembles = ensembles,
forecasts = forecasts2
)
return(results)
}
set.seed(123)
load_or_generate_data <- function(use_rda = TRUE, excel_path = NULL) {
if (use_rda) {
# Load data from .rda file in the "data" folder
dat <- dat
} else {
csv_path <- system.file("data", "up_sum_chk.csv", package = "SalmonForecasting")
if (file.exists(csv_path)) {
# Read data from CSV if it exists
up_sum_chk <- read.csv(csv_path)
} else {
# Generate data using make_dat_from_excel
up_sum_chk <- readxl::read_xlsx(excel_path, sheet = 1) %>%
brood_to_return() %>%
mutate(
abundance = Age4 + Age5 + Age6
) %>%
dplyr::select(year = ReturnYear, abundance, Age4, Jack = Age3) %>%
arrange(year)
#Save up_sum_chk as CSV
write.csv(up_sum_chk, csv_path, row.names = FALSE)
}
dat <- make_dat(dat1 = up_sum_chk)
}
return(dat)
}
# Set use_rda to TRUE to load from .rda file or FALSE to generate from Excel
use_rda <- TRUE
excel_path <- system.file("data", "SummerChinook.xlsx", package = "SalmonForecasting")
# Call the function
dat <- load_or_generate_data(use_rda = use_rda, excel_path = excel_path)
# Conditional assignment of covariates
if (use_rda) {
covariates <- c(
"lag1_log_JackOPI",
"lag1_log_SmAdj",
"lag1_NPGO",
"lag1_PDO",
"WSST_A",
"PDO.MJJ",
"MEI.OND",
"UWI.JAS",
"SST.AMJ",
"SSH.AMJ",
"UWI.SON"
)
} else {
covariates <- c(
"lag1_log_Jack",
"lag4_log_adults",
"lag5_log_adults",
"lag1_log_SAR",
"lag2_log_SAR",
"lag1_NPGO",
"lag1_PDO",
"lag2_NPGO",
"lag2_PDO",
"lag2_PC1",
"lag2_PC2",
"lag2_sp_phys_trans",
"pink_ind",
"lag1_log_socksmolt"
)
}
#=========
# Raw Data
#=========
yr_start<-1967
yr_end<-year(Sys.Date())
#dam="BON"
species="Coho"
#===========================
# Summarization for analysis
#===========================
date_start_analysis <- lubridate::ymd("1967/1/1")
date_end_analysis <- lubridate::ymd("2023/12/31")
forecast_period_start_m<-1 #this will be the month associated with the first month/day of the seasonal estimate period each year...can be after first data used to estimate it or the same
forecast_period_start_d<-1 #this will be the month day associated with the first month/day of the seasonal estimate period each year...can be after first data used to estimate it or the same
last_data<-Sys.Date()
#==================
#forecasting params
#==================
leave_yrs<- 31
TY_ensemble<-16
k<-1
covariates<-covariates
plot_results = F
first_forecast_period = 1
write_model_summaries = TRUE
find_best=T
#==============
#Ensemble Params
#===============
min_vars<-0
max_vars<-1
forecast_type<-"preseason"
stack_metric<-"MAPE"
num_models<-3
rolling_year_window<-15
inputs<-list(
yr_start = yr_start,
yr_end = yr_end,
species = species,
date_start_analysis = date_start_analysis,
date_end_analysis = date_end_analysis,
forecast_period_start_m = forecast_period_start_m,
forecast_period_start_d = forecast_period_start_d,
last_data = last_data,
leave_yrs = leave_yrs,
TY_ensemble = TY_ensemble,
k = k,
covariates = covariates,
plot_results = plot_results,
first_forecast_period = first_forecast_period,
write_model_summaries = write_model_summaries,
find_best = find_best,
min_vars = min_vars,
max_vars = max_vars,
forecast_type = forecast_type,
stack_metric = stack_metric,
num_models = num_models,
rolling_year_window
)
dat<-dat
if(find_best ==T){
best_covariates<-all_subsets(series=dat,covariates=covariates,min=min_vars,max=max_vars,type=forecast_type,fit=FALSE)
saveRDS(best_covariates,"best_covariates.rds")
}
best_covariates<-readRDS("best_covariates.rds")
model_list <- lapply(best_covariates[[1]], function(x) paste(x, collapse = " + ")) %>%
unlist() %>%
as_tibble() %>%
tibble::rownames_to_column() %>%
dplyr::rename(model = rowname, model_name = value)
fit_one_step<-F
if(fit_one_step){
results<-one_step_ahead(series=dat,
leave_yrs=leave_yrs,
TY_ensemble=TY_ensemble,
# covariates= best_covariates[[1]][best_covariates[[2]]$model_num[1:num_models]],
best_covariates[[1]],
first_forecast_period = first_forecast_period,
plot_results = plot_results,
write_model_summaries = write_model_summaries,
forecast_period_start_m =  forecast_period_start_m, #inclusive
forecast_period_start_d =  forecast_period_start_d, #inclusive
stack_metric = stack_metric,
k=k
)
write.csv(results,"outputs/forecasts.csv")
save(results,file="outputs/forecasts.rds")
}else{
results<-read_csv("forecasts.csv")
}
rp<-rolling_perf(results,dat,rolling_year_window,3,TY_ensemble,model_list)
# Call ensemble function
ens <- ensemble(
forecasts=rp$all_mods %>% group_by(year) %>% mutate(rank=rank(MAPE)) %>% ungroup,
series = dat,
TY_ensemble = TY_ensemble,
k = k,
slide = 15,
num_models=num_models,
stack_metric=stack_metric
)
expect_true(!is.null(ens))
# Assuming 'ens' is your data frame
print(head(ens, n = 10))
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
source("C:/Users/tjyua/OneDrive/Desktop/Jan22th24/salmonforecast/R/ensemble.R", echo=TRUE)
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
analyze_weights <- function(weight_vector) {
# Calculate mean and median weight
mean_weight <- mean(weight_vector)
median_weight <- median(weight_vector)
# Create a histogram of weights
hist(weight_vector, main="Weight Distribution", xlab="Weight (lbs)", col="lightblue", border="black")
# Return mean, median, and invisible plot
result <- list(
mean_weight = mean_weight,
median_weight = median_weight
)
invisible(result)  # invisible() is used to return only the result without printing the plot
}
# Example usage:
# Replace c(150, 160, 180, 140, 200) with your vector of people's weights in pounds
weights_result <- analyze_weights(c(150, 160, 180, 140, 200))
print(weights_result$mean_weight)
print(weights_result$median_weight)
knitr::opts_chunk$set(echo = TRUE)
# Install and load the tidyverse package
install.packages("tidyverse")
library(tidyverse)
# Load the Star Wars dataset
starwars_data <- starwars
# View the first few rows of the dataset
head(starwars_data)
# Filter data for "Human"
human_data <- starwars_data %>% filter(species == "Human")
# Create a new column for BMI
human_data <- human_data %>%
mutate(bmi = mass / ((height / 100)^2))
# View the resulting data
head(human_data)
# Group by "Species" and filter for "Human"
summary_human_data <- starwars_data %>%
group_by(Species) %>%
filter(Species == "Human") %>%
summarize(
mean_mass = mean(mass, na.rm = TRUE),
max_mass = max(mass, na.rm = TRUE),
min_mass = min(mass, na.rm = TRUE),
q1_mass = quantile(mass, 0.25, na.rm = TRUE),
q3_mass = quantile(mass, 0.75, na.rm = TRUE),
mean_height = mean(height, na.rm = TRUE),
max_height = max(height, na.rm = TRUE),
min_height = min(height, na.rm = TRUE),
q1_height = quantile(height, 0.25, na.rm = TRUE),
q3_height = quantile(height, 0.75, na.rm = TRUE)
)
# Group by "species" and filter for "Human"
summary_human_data <- starwars_data %>%
group_by(species) %>%
filter(species == "Human") %>%
summarize(
mean_mass = mean(mass, na.rm = TRUE),
max_mass = max(mass, na.rm = TRUE),
min_mass = min(mass, na.rm = TRUE),
q1_mass = quantile(mass, 0.25, na.rm = TRUE),
q3_mass = quantile(mass, 0.75, na.rm = TRUE),
mean_height = mean(height, na.rm = TRUE),
max_height = max(height, na.rm = TRUE),
min_height = min(height, na.rm = TRUE),
q1_height = quantile(height, 0.25, na.rm = TRUE),
q3_height = quantile(height, 0.75, na.rm = TRUE)
)
# View the summary data
print(summary_human_data)
mutate(data, new_column1 = expression1, new_column2 = expression2, ...)
# Add a new column "Balance_Income_Ratio" to Default dataset
default_with_ratio <- Default %>% mutate(Balance_Income_Ratio = balance / income)
# Load the ISLR package
library(ISLR)
# Check if the Default dataset is available
if (!exists("Default")) {
data("Default")
}
# Add a new column "Balance_Income_Ratio" to Default dataset
default_with_ratio <- Default %>% mutate(Balance_Income_Ratio = balance / income)
# Print the resulting data
print(default_with_ratio)
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
Table2 <- readRDS("C:/Users/tjyua/OneDrive/Desktop/Jan22th24/salmonforecast/tests/testthat/outputs/Table2.RDS")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
devtools::load_all(".")
library(SalmonForecasting)
library(SalmonForecasting)
